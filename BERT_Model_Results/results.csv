Parameter,Value
batch_size,32
dropout,0.06475614867026021
gradient_accumulation_steps,2
learning_rate,2.9714798637752895e-05
max_grad_norm,2
num_train_epochs,12
weight_decay,0.020532736091327948
return_dict,True
output_hidden_states,False
output_attentions,False
torchscript,False
torch_dtype,
use_bfloat16,False
tf_legacy_loss,False
pruned_heads,{}
tie_word_embeddings,True
chunk_size_feed_forward,0
is_encoder_decoder,False
is_decoder,False
cross_attention_hidden_size,
add_cross_attention,False
tie_encoder_decoder,False
max_length,20
min_length,0
do_sample,False
early_stopping,False
num_beams,1
num_beam_groups,1
diversity_penalty,0.0
temperature,1.0
top_k,50
top_p,1.0
typical_p,1.0
repetition_penalty,1.0
length_penalty,1.0
no_repeat_ngram_size,0
encoder_no_repeat_ngram_size,0
bad_words_ids,
num_return_sequences,1
output_scores,False
return_dict_in_generate,False
forced_bos_token_id,
forced_eos_token_id,
remove_invalid_values,False
exponential_decay_length_penalty,
suppress_tokens,
begin_suppress_tokens,
architectures,['BertForMaskedLM']
finetuning_task,
id2label,"{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7'}"
label2id,"{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7}"
tokenizer_class,
prefix,
bos_token_id,
pad_token_id,0
eos_token_id,
sep_token_id,
decoder_start_token_id,
task_specific_params,
problem_type,
transformers_version,4.48.3
gradient_checkpointing,True
model_type,bert
vocab_size,30522
hidden_size,768
num_hidden_layers,12
num_attention_heads,12
hidden_act,gelu
intermediate_size,3072
hidden_dropout_prob,0.06475614867026021
attention_probs_dropout_prob,0.06475614867026021
max_position_embeddings,512
type_vocab_size,2
initializer_range,0.02
layer_norm_eps,1e-12
position_embedding_type,absolute
use_cache,True
classifier_dropout,
output_dir,multi_output
overwrite_output_dir,False
do_train,False
do_eval,True
do_predict,False
eval_strategy,epoch
prediction_loss_only,False
per_device_train_batch_size,32
per_device_eval_batch_size,32
per_gpu_train_batch_size,
per_gpu_eval_batch_size,
eval_accumulation_steps,
eval_delay,0
torch_empty_cache_steps,
adam_beta1,0.9
adam_beta2,0.999
adam_epsilon,1e-08
max_steps,-1
lr_scheduler_type,cosine_with_restarts
lr_scheduler_kwargs,{}
warmup_ratio,0.0
warmup_steps,151
log_level,passive
log_level_replica,warning
log_on_each_node,True
logging_dir,multi_output/runs/Feb18_04-45-49_706f8e02309a
logging_strategy,epoch
logging_first_step,False
logging_steps,500
logging_nan_inf_filter,True
save_strategy,epoch
save_steps,500
save_total_limit,2
save_safetensors,True
save_on_each_node,False
save_only_model,False
restore_callback_states_from_checkpoint,False
no_cuda,False
use_cpu,False
use_mps_device,False
seed,42
data_seed,
jit_mode_eval,False
use_ipex,False
bf16,False
fp16,True
fp16_opt_level,O1
half_precision_backend,auto
bf16_full_eval,False
fp16_full_eval,False
tf32,
local_rank,0
ddp_backend,
tpu_num_cores,
tpu_metrics_debug,False
debug,[]
dataloader_drop_last,False
eval_steps,
dataloader_num_workers,0
dataloader_prefetch_factor,
past_index,-1
run_name,rosy-sweep-19
disable_tqdm,False
remove_unused_columns,True
label_names,
load_best_model_at_end,True
metric_for_best_model,eval_loss
greater_is_better,False
ignore_data_skip,False
fsdp,[]
fsdp_min_num_params,0
fsdp_config,"{'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}"
fsdp_transformer_layer_cls_to_wrap,
accelerator_config,"{'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}"
deepspeed,
label_smoothing_factor,0.0
optim,adamw_torch
optim_args,
adafactor,False
group_by_length,False
length_column_name,length
report_to,['wandb']
ddp_find_unused_parameters,
ddp_bucket_cap_mb,
ddp_broadcast_buffers,
dataloader_pin_memory,True
dataloader_persistent_workers,False
skip_memory_metrics,True
use_legacy_prediction_loop,False
push_to_hub,False
resume_from_checkpoint,
hub_model_id,
hub_strategy,every_save
hub_token,<HUB_TOKEN>
hub_private_repo,
hub_always_push,False
gradient_checkpointing_kwargs,
include_inputs_for_metrics,False
include_for_metrics,[]
eval_do_concat_batches,True
fp16_backend,auto
evaluation_strategy,epoch
push_to_hub_model_id,
push_to_hub_organization,
push_to_hub_token,<PUSH_TO_HUB_TOKEN>
mp_parameters,
auto_find_batch_size,False
full_determinism,False
torchdynamo,
ray_scope,last
ddp_timeout,1800
torch_compile,False
torch_compile_backend,
torch_compile_mode,
dispatch_batches,
split_batches,
include_tokens_per_second,False
include_num_input_tokens_seen,False
neftune_noise_alpha,
optim_target_modules,
batch_eval_metrics,False
eval_on_start,False
use_liger_kernel,False
eval_use_gather_object,False
average_tokens_across_devices,False
model/num_parameters,109488392
Epoch,Training Loss,Validation Loss,Accuracy,Precision,Recall,F1,Mcc
1,0.2567,0.1412781924009323,0.7267732267732268,0.6995418687507677,0.7267732267732268,0.6830047842138031,0.4870748387392635
2,0.0858,0.04685840755701065,0.8626373626373627,0.884135972817528,0.8626373626373627,0.8685742701239301,0.7741242979364544
3,0.024,0.033877596259117126,0.9055944055944056,0.9083298130865451,0.9055944055944056,0.9041691196676072,0.8309537083085277
4,0.0079,0.03140394762158394,0.9175824175824175,0.9202312191669453,0.9175824175824175,0.918102596912468,0.8545765022243081
5,0.0032,0.032225869596004486,0.9245754245754245,0.9251141822605644,0.9245754245754245,0.9245892014196948,0.8661546485438423
6,0.0018,0.033072277903556824,0.9250749250749251,0.9247126115899996,0.9250749250749251,0.9237854254370704,0.8656552015578537
7,0.0011,0.031009087339043617,0.9340659340659341,0.9336513732488387,0.9340659340659341,0.9334750506113603,0.881970363965251
8,0.0006,0.03259903937578201,0.9250749250749251,0.9277743528050152,0.9250749250749251,0.9258926034408023,0.8686881416105701
9,0.0004,0.033150315284729004,0.9235764235764236,0.926374455716566,0.9235764235764236,0.9242792774551806,0.8658309585472543
10,0.0003,0.03268831968307495,0.9285714285714286,0.9302624847752476,0.9285714285714286,0.9290219550520838,0.8739829312784761
11,,0.031009087339043617,0.9340659340659341,0.9336513732488387,0.9340659340659341,0.9334750506113603,0.881970363965251
12,,0.031009087339043617,0.9340659340659341,0.9336513732488387,0.9340659340659341,0.9334750506113603,0.881970363965251
