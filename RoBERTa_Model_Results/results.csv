Parameter,Value
batch_size,32
dropout,0.058529503694848654
gradient_accumulation_steps,4
learning_rate,4.300495514427046e-05
max_grad_norm,2
num_train_epochs,12
weight_decay,0.04230420352896582
return_dict,True
output_hidden_states,False
output_attentions,False
torchscript,False
torch_dtype,
use_bfloat16,False
tf_legacy_loss,False
pruned_heads,{}
tie_word_embeddings,True
chunk_size_feed_forward,0
is_encoder_decoder,False
is_decoder,False
cross_attention_hidden_size,
add_cross_attention,False
tie_encoder_decoder,False
max_length,20
min_length,0
do_sample,False
early_stopping,False
num_beams,1
num_beam_groups,1
diversity_penalty,0.0
temperature,1.0
top_k,50
top_p,1.0
typical_p,1.0
repetition_penalty,1.0
length_penalty,1.0
no_repeat_ngram_size,0
encoder_no_repeat_ngram_size,0
bad_words_ids,
num_return_sequences,1
output_scores,False
return_dict_in_generate,False
forced_bos_token_id,
forced_eos_token_id,
remove_invalid_values,False
exponential_decay_length_penalty,
suppress_tokens,
begin_suppress_tokens,
architectures,['RobertaForMaskedLM']
finetuning_task,
id2label,"{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7'}"
label2id,"{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7}"
tokenizer_class,
prefix,
bos_token_id,0
pad_token_id,1
eos_token_id,2
sep_token_id,
decoder_start_token_id,
task_specific_params,
problem_type,
transformers_version,4.48.3
model_type,roberta
vocab_size,50265
hidden_size,768
num_hidden_layers,12
num_attention_heads,12
hidden_act,gelu
intermediate_size,3072
hidden_dropout_prob,0.058529503694848654
attention_probs_dropout_prob,0.058529503694848654
max_position_embeddings,514
type_vocab_size,1
initializer_range,0.02
layer_norm_eps,1e-05
position_embedding_type,absolute
use_cache,True
classifier_dropout,
output_dir,multi_output
overwrite_output_dir,False
do_train,False
do_eval,True
do_predict,False
eval_strategy,epoch
prediction_loss_only,False
per_device_train_batch_size,32
per_device_eval_batch_size,32
per_gpu_train_batch_size,
per_gpu_eval_batch_size,
eval_accumulation_steps,
eval_delay,0
torch_empty_cache_steps,
adam_beta1,0.9
adam_beta2,0.999
adam_epsilon,1e-08
max_steps,-1
lr_scheduler_type,cosine_with_restarts
lr_scheduler_kwargs,{}
warmup_ratio,0.0
warmup_steps,75
log_level,passive
log_level_replica,warning
log_on_each_node,True
logging_dir,multi_output/runs/Feb18_02-22-25_62a9ea51b781
logging_strategy,epoch
logging_first_step,False
logging_steps,500
logging_nan_inf_filter,True
save_strategy,epoch
save_steps,500
save_total_limit,2
save_safetensors,True
save_on_each_node,False
save_only_model,False
restore_callback_states_from_checkpoint,False
no_cuda,False
use_cpu,False
use_mps_device,False
seed,42
data_seed,
jit_mode_eval,False
use_ipex,False
bf16,False
fp16,True
fp16_opt_level,O1
half_precision_backend,auto
bf16_full_eval,False
fp16_full_eval,False
tf32,
local_rank,0
ddp_backend,
tpu_num_cores,
tpu_metrics_debug,False
debug,[]
dataloader_drop_last,False
eval_steps,
dataloader_num_workers,0
dataloader_prefetch_factor,
past_index,-1
run_name,comic-sweep-11
disable_tqdm,False
remove_unused_columns,True
label_names,
load_best_model_at_end,True
metric_for_best_model,eval_loss
greater_is_better,False
ignore_data_skip,False
fsdp,[]
fsdp_min_num_params,0
fsdp_config,"{'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}"
fsdp_transformer_layer_cls_to_wrap,
accelerator_config,"{'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}"
deepspeed,
label_smoothing_factor,0.0
optim,adamw_torch
optim_args,
adafactor,False
group_by_length,False
length_column_name,length
report_to,['wandb']
ddp_find_unused_parameters,
ddp_bucket_cap_mb,
ddp_broadcast_buffers,
dataloader_pin_memory,True
dataloader_persistent_workers,False
skip_memory_metrics,True
use_legacy_prediction_loop,False
push_to_hub,False
resume_from_checkpoint,
hub_model_id,
hub_strategy,every_save
hub_token,<HUB_TOKEN>
hub_private_repo,
hub_always_push,False
gradient_checkpointing,True
gradient_checkpointing_kwargs,
include_inputs_for_metrics,False
include_for_metrics,[]
eval_do_concat_batches,True
fp16_backend,auto
evaluation_strategy,epoch
push_to_hub_model_id,
push_to_hub_organization,
push_to_hub_token,<PUSH_TO_HUB_TOKEN>
mp_parameters,
auto_find_batch_size,False
full_determinism,False
torchdynamo,
ray_scope,last
ddp_timeout,1800
torch_compile,False
torch_compile_backend,
torch_compile_mode,
dispatch_batches,
split_batches,
include_tokens_per_second,False
include_num_input_tokens_seen,False
neftune_noise_alpha,
optim_target_modules,
batch_eval_metrics,False
eval_on_start,False
use_liger_kernel,False
eval_use_gather_object,False
average_tokens_across_devices,False
model/num_parameters,124651784
Epoch,Training Loss,Validation Loss,Accuracy,Precision,Recall,F1,Mcc
1,0.2399,0.104190394282341,0.7777222777222778,0.8169624598162005,0.7777222777222778,0.7799721422746974,0.6335768964570373
2,0.0481,0.039399996399879456,0.8796203796203796,0.9018549835791133,0.8796203796203796,0.8854912764067111,0.8036469271488462
3,0.02,0.03518366441130638,0.8966033966033966,0.9112645321212212,0.8966033966033966,0.9007017009025394,0.822680048902364
4,0.0101,0.03420916199684143,0.9140859140859141,0.9176254835838769,0.9140859140859141,0.9137284173732133,0.8483366649510775
5,0.005,0.031090237200260162,0.929070929070929,0.9306451465600633,0.929070929070929,0.9294650752795464,0.874436848028118
6,0.002,0.03316996991634369,0.9210789210789211,0.9215071586327368,0.9210789210789211,0.9197326193794799,0.8590643368708115
7,0.0012,0.03315727040171623,0.926073926073926,0.9263078310164512,0.926073926073926,0.9255692231165965,0.8682540213278669
8,0.0007,0.032364655286073685,0.9265734265734266,0.9281958436607937,0.9265734265734266,0.9271290618358919,0.8706217463010998
9,,0.031090237200260162,0.929070929070929,0.9306451465600633,0.929070929070929,0.9294650752795464,0.874436848028118
