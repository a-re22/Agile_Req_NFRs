Parameter,Value
batch_size,8
dropout,0.10592329650872234
gradient_accumulation_steps,2
learning_rate,1.228609193617499e-05
max_grad_norm,2
num_train_epochs,12
weight_decay,0.029873264756975625
vocab_size,30522
max_position_embeddings,512
sinusoidal_pos_embds,False
n_layers,6
n_heads,12
dim,768
hidden_dim,3072
attention_dropout,0.1
activation,gelu
initializer_range,0.02
qa_dropout,0.1
seq_classif_dropout,0.2
return_dict,True
output_hidden_states,False
output_attentions,False
torchscript,False
torch_dtype,
use_bfloat16,False
tf_legacy_loss,False
pruned_heads,{}
tie_word_embeddings,True
chunk_size_feed_forward,0
is_encoder_decoder,False
is_decoder,False
cross_attention_hidden_size,
add_cross_attention,False
tie_encoder_decoder,False
max_length,20
min_length,0
do_sample,False
early_stopping,False
num_beams,1
num_beam_groups,1
diversity_penalty,0.0
temperature,1.0
top_k,50
top_p,1.0
typical_p,1.0
repetition_penalty,1.0
length_penalty,1.0
no_repeat_ngram_size,0
encoder_no_repeat_ngram_size,0
bad_words_ids,
num_return_sequences,1
output_scores,False
return_dict_in_generate,False
forced_bos_token_id,
forced_eos_token_id,
remove_invalid_values,False
exponential_decay_length_penalty,
suppress_tokens,
begin_suppress_tokens,
architectures,['DistilBertForMaskedLM']
finetuning_task,
id2label,"{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7'}"
label2id,"{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7}"
tokenizer_class,
prefix,
bos_token_id,
pad_token_id,0
eos_token_id,
sep_token_id,
decoder_start_token_id,
task_specific_params,
problem_type,
transformers_version,4.48.3
model_type,distilbert
tie_weights_,True
output_dir,multi_output
overwrite_output_dir,False
do_train,False
do_eval,True
do_predict,False
eval_strategy,epoch
prediction_loss_only,False
per_device_train_batch_size,8
per_device_eval_batch_size,8
per_gpu_train_batch_size,
per_gpu_eval_batch_size,
eval_accumulation_steps,
eval_delay,0
torch_empty_cache_steps,
adam_beta1,0.9
adam_beta2,0.999
adam_epsilon,1e-08
max_steps,-1
lr_scheduler_type,cosine_with_restarts
lr_scheduler_kwargs,{}
warmup_ratio,0.0
warmup_steps,601
log_level,passive
log_level_replica,warning
log_on_each_node,True
logging_dir,multi_output/runs/Feb18_04-28-40_20f38a896a06
logging_strategy,epoch
logging_first_step,False
logging_steps,500
logging_nan_inf_filter,True
save_strategy,epoch
save_steps,500
save_total_limit,2
save_safetensors,True
save_on_each_node,False
save_only_model,False
restore_callback_states_from_checkpoint,False
no_cuda,False
use_cpu,False
use_mps_device,False
seed,42
data_seed,
jit_mode_eval,False
use_ipex,False
bf16,False
fp16,True
fp16_opt_level,O1
half_precision_backend,auto
bf16_full_eval,False
fp16_full_eval,False
tf32,
local_rank,0
ddp_backend,
tpu_num_cores,
tpu_metrics_debug,False
debug,[]
dataloader_drop_last,False
eval_steps,
dataloader_num_workers,0
dataloader_prefetch_factor,
past_index,-1
run_name,radiant-sweep-19
disable_tqdm,False
remove_unused_columns,True
label_names,
load_best_model_at_end,True
metric_for_best_model,eval_loss
greater_is_better,False
ignore_data_skip,False
fsdp,[]
fsdp_min_num_params,0
fsdp_config,"{'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}"
fsdp_transformer_layer_cls_to_wrap,
accelerator_config,"{'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}"
deepspeed,
label_smoothing_factor,0.0
optim,adamw_torch
optim_args,
adafactor,False
group_by_length,False
length_column_name,length
report_to,['wandb']
ddp_find_unused_parameters,
ddp_bucket_cap_mb,
ddp_broadcast_buffers,
dataloader_pin_memory,True
dataloader_persistent_workers,False
skip_memory_metrics,True
use_legacy_prediction_loop,False
push_to_hub,False
resume_from_checkpoint,
hub_model_id,
hub_strategy,every_save
hub_token,<HUB_TOKEN>
hub_private_repo,
hub_always_push,False
gradient_checkpointing,True
gradient_checkpointing_kwargs,
include_inputs_for_metrics,False
include_for_metrics,[]
eval_do_concat_batches,True
fp16_backend,auto
evaluation_strategy,epoch
push_to_hub_model_id,
push_to_hub_organization,
push_to_hub_token,<PUSH_TO_HUB_TOKEN>
mp_parameters,
auto_find_batch_size,False
full_determinism,False
torchdynamo,
ray_scope,last
ddp_timeout,1800
torch_compile,False
torch_compile_backend,
torch_compile_mode,
dispatch_batches,
split_batches,
include_tokens_per_second,False
include_num_input_tokens_seen,False
neftune_noise_alpha,
optim_target_modules,
batch_eval_metrics,False
eval_on_start,False
use_liger_kernel,False
eval_use_gather_object,False
average_tokens_across_devices,False
model/num_parameters,66959624
Epoch,Training Loss,Validation Loss,Accuracy,Precision,Recall,F1,Mcc
1,0.2214,0.11141147464513779,0.7787212787212787,0.7708872037154212,0.7787212787212787,0.7497150013550502,0.5789402106094274
2,0.0654,0.04361928999423981,0.8706293706293706,0.8932042440519432,0.8706293706293706,0.8756993120144729,0.7857339652525175
3,0.0229,0.0333198718726635,0.9045954045954046,0.9068758387655095,0.9045954045954046,0.9052876338370722,0.8326140071905729
4,0.0108,0.03326253592967987,0.9055944055944056,0.9137744521393546,0.9055944055944056,0.9076799320524369,0.8397637700096975
5,0.0062,0.035654738545417786,0.903096903096903,0.9111962843434935,0.903096903096903,0.9054642687698495,0.8345460380675727
6,0.003,0.037529539316892624,0.9215784215784216,0.9212481315627228,0.9215784215784216,0.9194232041759117,0.8582571710447728
7,0.0018,0.03269612416625023,0.9245754245754245,0.9256225518252899,0.9245754245754245,0.9247722200575433,0.8674970279049762
8,0.0013,0.03314553573727608,0.9215784215784216,0.9230821589725697,0.9215784215784216,0.9218081593976067,0.8622840693538519
9,0.0008,0.03406412899494171,0.9210789210789211,0.9224678198167896,0.9210789210789211,0.9213811731404689,0.8613467799861099
10,0.0007,0.03507879748940468,0.9225774225774226,0.9247444523538229,0.9225774225774226,0.9230689187538412,0.8645457168801439
11,,0.03269612416625023,0.9245754245754245,0.9256225518252899,0.9245754245754245,0.9247722200575433,0.8674970279049762
