Parameter,Value
batch_size,8
dropout,0.09616786785046327
gradient_accumulation_steps,4
learning_rate,4.5068012486482445e-05
max_grad_norm,2
num_train_epochs,12
weight_decay,0.038132825200275104
vocab_size,32000
d_model,768
n_layer,12
n_head,12
d_head,64
ff_activation,gelu
d_inner,3072
untie_r,True
attn_type,bi
initializer_range,0.02
layer_norm_eps,1e-12
mem_len,
reuse_len,
bi_data,False
clamp_len,-1
same_length,False
summary_type,last
summary_use_proj,True
summary_activation,tanh
summary_last_dropout,0.1
start_n_top,5
end_n_top,5
bos_token_id,1
pad_token_id,5
eos_token_id,2
use_mems_eval,True
use_mems_train,False
return_dict,True
output_hidden_states,False
output_attentions,False
torchscript,False
torch_dtype,
use_bfloat16,False
tf_legacy_loss,False
pruned_heads,{}
tie_word_embeddings,True
chunk_size_feed_forward,0
is_encoder_decoder,False
is_decoder,False
cross_attention_hidden_size,
add_cross_attention,False
tie_encoder_decoder,False
max_length,20
min_length,0
do_sample,False
early_stopping,False
num_beams,1
num_beam_groups,1
diversity_penalty,0.0
temperature,1.0
top_k,50
top_p,1.0
typical_p,1.0
repetition_penalty,1.0
length_penalty,1.0
no_repeat_ngram_size,0
encoder_no_repeat_ngram_size,0
bad_words_ids,
num_return_sequences,1
output_scores,False
return_dict_in_generate,False
forced_bos_token_id,
forced_eos_token_id,
remove_invalid_values,False
exponential_decay_length_penalty,
suppress_tokens,
begin_suppress_tokens,
architectures,['XLNetLMHeadModel']
finetuning_task,
id2label,"{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7'}"
label2id,"{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7}"
tokenizer_class,
prefix,
sep_token_id,
decoder_start_token_id,
task_specific_params,"{'text-generation': {'do_sample': True, 'max_length': 250}}"
problem_type,
transformers_version,4.47.1
model_type,xlnet
output_dir,multi_output
overwrite_output_dir,False
do_train,False
do_eval,True
do_predict,False
eval_strategy,epoch
prediction_loss_only,False
per_device_train_batch_size,8
per_device_eval_batch_size,8
per_gpu_train_batch_size,
per_gpu_eval_batch_size,
eval_accumulation_steps,
eval_delay,0
torch_empty_cache_steps,
adam_beta1,0.9
adam_beta2,0.999
adam_epsilon,1e-08
max_steps,-1
lr_scheduler_type,cosine_with_restarts
lr_scheduler_kwargs,{}
warmup_ratio,0.0
warmup_steps,301
log_level,passive
log_level_replica,warning
log_on_each_node,True
logging_dir,multi_output/runs/Feb18_09-31-45_5fb1c30aa88a
logging_strategy,epoch
logging_first_step,False
logging_steps,500
logging_nan_inf_filter,True
save_strategy,epoch
save_steps,500
save_total_limit,2
save_safetensors,True
save_on_each_node,False
save_only_model,False
restore_callback_states_from_checkpoint,False
no_cuda,False
use_cpu,False
use_mps_device,False
seed,42
data_seed,
jit_mode_eval,False
use_ipex,False
bf16,False
fp16,True
fp16_opt_level,O1
half_precision_backend,auto
bf16_full_eval,False
fp16_full_eval,False
tf32,
local_rank,0
ddp_backend,
tpu_num_cores,
tpu_metrics_debug,False
debug,[]
dataloader_drop_last,False
eval_steps,
dataloader_num_workers,0
dataloader_prefetch_factor,
past_index,-1
run_name,icy-sweep-17
disable_tqdm,False
remove_unused_columns,True
label_names,
load_best_model_at_end,True
metric_for_best_model,eval_loss
greater_is_better,False
ignore_data_skip,False
fsdp,[]
fsdp_min_num_params,0
fsdp_config,"{'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}"
fsdp_transformer_layer_cls_to_wrap,
accelerator_config,"{'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}"
deepspeed,
label_smoothing_factor,0.0
optim,adamw_torch
optim_args,
adafactor,False
group_by_length,False
length_column_name,length
report_to,['wandb']
ddp_find_unused_parameters,
ddp_bucket_cap_mb,
ddp_broadcast_buffers,
dataloader_pin_memory,True
dataloader_persistent_workers,False
skip_memory_metrics,True
use_legacy_prediction_loop,False
push_to_hub,False
resume_from_checkpoint,
hub_model_id,
hub_strategy,every_save
hub_token,<HUB_TOKEN>
hub_private_repo,
hub_always_push,False
gradient_checkpointing,False
gradient_checkpointing_kwargs,
include_inputs_for_metrics,False
include_for_metrics,[]
eval_do_concat_batches,True
fp16_backend,auto
evaluation_strategy,epoch
push_to_hub_model_id,
push_to_hub_organization,
push_to_hub_token,<PUSH_TO_HUB_TOKEN>
mp_parameters,
auto_find_batch_size,False
full_determinism,False
torchdynamo,
ray_scope,last
ddp_timeout,1800
torch_compile,False
torch_compile_backend,
torch_compile_mode,
dispatch_batches,
split_batches,
include_tokens_per_second,False
include_num_input_tokens_seen,False
neftune_noise_alpha,
optim_target_modules,
batch_eval_metrics,False
eval_on_start,False
use_liger_kernel,False
eval_use_gather_object,False
average_tokens_across_devices,False
model/num_parameters,117315080
Epoch,Training Loss,Validation Loss,Accuracy,Precision,Recall,F1,Mcc
1,0.6326,0.053392812609672546,0.8656343656343657,0.8654546355809422,0.8656343656343657,0.8534683560837393,0.752707022456487
2,0.1967,0.04215632751584053,0.8796203796203796,0.8956516067361141,0.8796203796203796,0.8830051817393793,0.7986179581745785
3,0.1014,0.03548889234662056,0.903096903096903,0.9064258114744318,0.903096903096903,0.9023262486526513,0.829992491542586
4,0.0519,0.0325460210442543,0.9240759240759241,0.9243104933549641,0.9240759240759241,0.9238567840112208,0.8651665064227058
5,0.0299,0.04975612461566925,0.8921078921078921,0.9040994489137799,0.8921078921078921,0.8956881879556214,0.8194025028140062
6,0.0144,0.042686302214860916,0.9270729270729271,0.926811595719175,0.9270729270729271,0.9260344399201333,0.8693727743334937
7,0.0114,0.04306494817137718,0.9250749250749251,0.9255963244942219,0.9250749250749251,0.924874322060649,0.867455341616974
8,,0.0325460210442543,0.9240759240759241,0.9243104933549641,0.9240759240759241,0.9238567840112208,0.8651665064227058
